## Summary

This example shows the use of c2numpy 1.1 (C interface) in CMSSW framework. The directory contains files and directories generated by the `mkedanlzr` script of CMSSW; we assume readers are familiar with that environment. If not you can follow up the CMSSW standard procedure [ho to write EDAnalyzer](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookWriteFrameworkModule)

## Getting started

To reproduce it, run

```bash
cmsrel CMSSW_8_0_16
cd CMSSW_8_0_16/src
cmsenv
```

and copy in the `Demo` directory with its contents. Then

```bash
scram build
```

to compile it.

## What was added

We added c2numpy to this analyzer by copying the header file into `Demo/DemoAnalyzer/interface/c2numpy.h` (for some projects, you need to create the `interface` directory first). Then it was called by

  1. Adding `#include "Demo/DemoAnalyzer/interface/c2numpy.h"` at the top of `Demo/DemoAnalyzer/plugins/DemoAnalyzer.cc`.
  
  2. Adding `c2numpy_writer writer;` as member data in the `DemoAnalyzer` class.
  
  3. Adding a

     ```c++
c2numpy_init(&writer, "output/trackparams", 1000);
c2numpy_addcolumn(&writer, "pt", C2NUMPY_FLOAT64);
c2numpy_addcolumn(&writer, "eta", C2NUMPY_FLOAT64);
c2numpy_addcolumn(&writer, "phi", C2NUMPY_FLOAT64);
c2numpy_addcolumn(&writer, "dxy", C2NUMPY_FLOAT64);
c2numpy_addcolumn(&writer, "dz", C2NUMPY_FLOAT64);
```

     block in the `DemoAnalyzer` constructor.
  4. Adding a

     ```c++
for (auto track = tracks->cbegin();  track != tracks->end();  ++track) {
  c2numpy_float64(&writer, track->pt());
  c2numpy_float64(&writer, track->eta());
  c2numpy_float64(&writer, track->phi());
  c2numpy_float64(&writer, track->dxy());
  c2numpy_float64(&writer, track->dz());
}
```
     block in the `DemoAnalyzer::analyze` method.
  5. **This is important!** Adding a `c2numpy_close(&writer);` statement in `DemoAnalyzer::endJob()`.

The future C++ API will make this (easily forgotten) step 5 unnecessary.

## How to run it

If you're testing this on CERN LXPLUS or another AFS mounted machine (to see the dataset at `/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root`), simply do

```bash
mkdir output
cmsRun Demo/DemoAnalyzer/python/ConfFile_cfg.py
```

When CMSSW runs over the 10-event dataset, it produces seven files in `output` named `trackparams0.npy` through `trackparams6.npy`. To make one file, increase the number of rows per file (currently 1000) in the `c2numpy_init` call, recompile, and rerun.

These files should match the ones in this repository.

## What to do with the output

You can load any of the files in a Python session. Start a Python prompt and do

```python
import numpy as np
>>> trackparams0 = np.load("output/trackparams0.npy")
```

to get the recarray. This array represents a table of data with typed, named columns (just like a flat ROOT NTuple or a Pandas DataFrame). You can access it a column at a time:

```python
>>> trackparams0["pt"]
array([  0.58615864,   0.47248294,   1.74911408,   1.04643586,
         0.47096605,   0.59780441,   0.88029248,   3.06881774,
         0.66100567,   1.33790879,   0.65042406,   0.60015372,
         0.86379886,   0.57943386,   0.53491224,   1.0408088 ,
         1.55475394,   1.02831484,   0.47554129,   1.45042064,
...

# to see array shape we can use built in numpy functionality
>>> print np.shape(trackparams0)
(1000,)
```

a row at a time:

```python
>>> trackparams0[100]
(1.384070332555011, 0.18779495889500655, -1.6946575614328503, 0.19390893548491872, 0.39640133641298614)
>>> trackparams0.dtype.names
('pt', 'eta', 'phi', 'dxy', 'dz')
>>> trackparams0.dtype["pt"]
dtype('float64')
```

or any combination:

```python
>>> trackparams0["pt"][100:150]
array([ 1.38407033,  0.5997181 ,  2.64444269,  1.17549855,  2.81446767,
        1.41189475,  2.28960126,  0.56264374,  1.43745931,  0.78332034,
        1.34970658,  0.81638258,  1.52328245,  0.60757518,  1.1785421 ,
        0.81772201,  0.74880223,  0.69483397,  1.61993328,  0.59333001,
        1.15131582,  0.49265859,  2.57502517,  1.07178172,  0.67694687,
        0.463539  ,  1.4918132 ,  0.80275165,  0.86756021,  0.66508061,
        0.46361261,  1.27049738,  0.80131647,  0.59497201,  0.48336115,
        0.44257204,  0.47842197,  0.79463124,  0.77059007,  1.04439877,
        0.78301175,  0.53878087,  0.66803512,  1.03883587,  1.72249824,
        1.22609581,  0.96652589,  0.45579263,  0.62061272,  0.60283977])
```

## Importing to a Pandas DataFrame

Let's convert this into pandas DataFrame and see how we can access the data

```python
>>> import pandas as pd
>>> df = pd.DataFrame(trackparams0)
>>> # access first tree rows
>>> print df.iloc[:3]
       dxy        dz
0  0.586159  0.088209 -3.124979 -0.374269  1.817592
1  0.472483  0.482395 -2.954974 -0.396636  9.588100
2  1.749114 -0.549223 -3.007367 -0.363395 -8.003905
```

Pandas dataframe provides a convenient way to work with dataframes,
for instance you may inspect the dataframe and see its shape, columns, etc.

```python
>>> print df.columns
Index([u'pt', u'eta', u'phi', u'dxy', u'dz'], dtype='object')
```

## Ready for Machine Learning (ML) challenge

Now, let's use our dataframe for ML studies, such as classification problem.
To do that we need a *labels* which will be used by ML classifier.
For simplicity we'll randomly generate it:

```python
>>> import random
>>> # we have 1000 rows, therefore we need 1000 labels
>>> labels = [random.randint(0,1) for _ in range(1000)]
```

At this step we're ready to apply any ML techniques and for demonstration
purposes we'll use well-known python [scikit-learn library](http://scikit-learn.org/).

```python
>>> # import scikit learn RandomForest classifier
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.cross_validation import train_test_split

>>> # split data into train/validation/test sets
>>> x_train, x_rest, y_train, y_rest = train_test_split(df, labels, test_size=0.3, random_state=12345)

>>> # train the model and generate predictions
>>> fit = clf.fit(x_train, y_train)
>>> predictions = fit.predict(x_rest)
```

At this step our predictions are ready. Of course, in this simple example
they're meaningless, but this example shows full pipeline how someone
can go from CMSSW root based data into ML pipeline in very simple steps.
